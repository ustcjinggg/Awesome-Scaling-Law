# Awesome-Scaling-Law

## OpenAI Kaplan et al. 2020提出[Scaling Laws](https://arxiv.org/pdf/2001.08361.pdf)
#### LLM模型性能与三个主要因素（模型参数量N、数据集大小D和训练计算量C）的幂律关系。给定计算预算c，经验性地提出缩放规律的三个基本公式
![image](https://github.com/ustcjinggg/Awesome-Scaling-Law/assets/38032906/915c9717-36d0-426c-a0cb-85da8ab488ab)

## DeepMind Hoffmann et al. 2022提出 [Chinchilla Scaling](https://arxiv.org/pdf/2203.15556.pdf)
#### 改变了更大范围的模型大小（从70M到16B）和数据大小（从5B到500B tokens），并拟合了具有不同系数的类似缩放规律
![image](https://github.com/ustcjinggg/Awesome-Scaling-Law/assets/38032906/cee62021-b194-4363-a7ef-8903ea1cdeb5)
